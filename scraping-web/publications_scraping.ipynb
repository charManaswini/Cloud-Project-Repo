{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPOYhdzK0zAE",
        "outputId": "6ec09236-b660-44fd-8472-91b14ab4fb35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import openpyxl\n",
        "import re\n",
        "\n",
        "def sanitize_text(text):\n",
        "    # Remove any non-ASCII characters\n",
        "    sanitized_text = ''.join(char for char in text if ord(char) < 128)\n",
        "    # Replace illegal characters with a placeholder\n",
        "    sanitized_text = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', sanitized_text)\n",
        "    return sanitized_text\n",
        "\n",
        "def scrape_page(page_url):\n",
        "    response = requests.get(page_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    papers = []\n",
        "\n",
        "    # Find all elements with class \"publications-inner\"\n",
        "    publication_elements = soup.find_all('div', class_='publications-inner')\n",
        "\n",
        "    for pub_elem in publication_elements:\n",
        "        # Extract title and URL\n",
        "        title_elem = pub_elem.find('div', class_='article-title').find('a')\n",
        "        title = title_elem.text.strip()\n",
        "        url = title_elem['href']\n",
        "\n",
        "        # Extract author names\n",
        "        author_elem = pub_elem.find('p').find('strong')\n",
        "        if author_elem:\n",
        "            author = author_elem.next_sibling.strip()\n",
        "        else:\n",
        "            author = \"N/A\"  # If author information is not found\n",
        "\n",
        "        papers.append({'title': title, 'url': url, 'author': author})\n",
        "\n",
        "    return papers\n",
        "\n",
        "# Scrape data from specified number of pages\n",
        "num_pages = 11\n",
        "all_papers = []\n",
        "for page_num in range(1, num_pages + 1):\n",
        "    page_url = f\"https://www.amrita.edu/publications/page/{page_num}/\"\n",
        "    all_papers.extend(scrape_page(page_url))\n",
        "    print(f\"Scraped page {page_num}\")\n",
        "\n",
        "# Create Excel workbook and sheet\n",
        "workbook = openpyxl.Workbook()\n",
        "sheet = workbook.active\n",
        "\n",
        "# Write headers\n",
        "sheet.append(['Title', 'URL', 'Author'])\n",
        "\n",
        "# Write data\n",
        "for paper in all_papers:\n",
        "    title = sanitize_text(paper.get('title', ''))\n",
        "    url = sanitize_text(paper.get('url', ''))\n",
        "    author = sanitize_text(paper.get('author', ''))\n",
        "    sheet.append([title, url, author])\n",
        "\n",
        "# Save the workbook\n",
        "workbook.save(\"publications.xlsx\")\n",
        "\n",
        "# Download the Excel sheet\n",
        "from google.colab import files\n",
        "files.download(\"publications.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "_Nd8vFmJDOOf",
        "outputId": "c7e489e9-da2f-4db6-d5b8-8498f848db1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped page 1\n",
            "Scraped page 2\n",
            "Scraped page 3\n",
            "Scraped page 4\n",
            "Scraped page 5\n",
            "Scraped page 6\n",
            "Scraped page 7\n",
            "Scraped page 8\n",
            "Scraped page 9\n",
            "Scraped page 10\n",
            "Scraped page 11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b433cbef-c642-4204-9fd8-c9a3f2585649\", \"publications.xlsx\", 16756)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import openpyxl\n",
        "import re\n",
        "\n",
        "def sanitize_text(text):\n",
        "    # Remove any non-ASCII characters\n",
        "    sanitized_text = ''.join(char for char in text if ord(char) < 128)\n",
        "    # Replace illegal characters with a placeholder\n",
        "    sanitized_text = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', sanitized_text)\n",
        "    return sanitized_text\n",
        "\n",
        "def scrape_page(page_url):\n",
        "    response = requests.get(page_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    papers = []\n",
        "\n",
        "    # Find all elements with class \"article-cntnt\"\n",
        "    article_contents = soup.find_all('div', class_='article-cntnt')\n",
        "\n",
        "    for article_content in article_contents:\n",
        "        # Extract title and URL\n",
        "        title_elem = article_content.find('div', class_='article-title').find('a')\n",
        "        title = title_elem.text.strip()\n",
        "        url = title_elem['href']\n",
        "\n",
        "        # Extract author names\n",
        "        author_links = article_content.find_all('a')\n",
        "        author_names = [author_link.text.strip() for author_link in author_links if '/faculty/' in author_link.get('href', '')]\n",
        "        author = \", \".join(author_names)\n",
        "\n",
        "        papers.append({'title': title, 'url': url, 'author': author})\n",
        "\n",
        "    return papers\n",
        "\n",
        "# Scrape data from specified number of pages\n",
        "num_pages = 50\n",
        "all_papers = []\n",
        "for page_num in range(1, num_pages + 1):\n",
        "    page_url = f\"https://www.amrita.edu/publications/page/{page_num}/\"\n",
        "    all_papers.extend(scrape_page(page_url))\n",
        "    print(f\"Scraped page {page_num}\")\n",
        "\n",
        "# Create Excel workbook and sheet\n",
        "workbook = openpyxl.Workbook()\n",
        "sheet = workbook.active\n",
        "\n",
        "# Write headers\n",
        "sheet.append(['Title', 'URL', 'Author'])\n",
        "\n",
        "# Write data\n",
        "for paper in all_papers:\n",
        "    title = sanitize_text(paper.get('title', ''))\n",
        "    url = sanitize_text(paper.get('url', ''))\n",
        "    author = sanitize_text(paper.get('author', ''))\n",
        "    sheet.append([title, url, author])\n",
        "\n",
        "# Save the workbook\n",
        "workbook.save(\"publications.xlsx\")\n",
        "\n",
        "# Download the Excel sheet\n",
        "from google.colab import files\n",
        "files.download(\"publications.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "DR4yxZVuKBtL",
        "outputId": "4443a4a2-3983-4865-ecc1-cdb1466d1de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped page 1\n",
            "Scraped page 2\n",
            "Scraped page 3\n",
            "Scraped page 4\n",
            "Scraped page 5\n",
            "Scraped page 6\n",
            "Scraped page 7\n",
            "Scraped page 8\n",
            "Scraped page 9\n",
            "Scraped page 10\n",
            "Scraped page 11\n",
            "Scraped page 12\n",
            "Scraped page 13\n",
            "Scraped page 14\n",
            "Scraped page 15\n",
            "Scraped page 16\n",
            "Scraped page 17\n",
            "Scraped page 18\n",
            "Scraped page 19\n",
            "Scraped page 20\n",
            "Scraped page 21\n",
            "Scraped page 22\n",
            "Scraped page 23\n",
            "Scraped page 24\n",
            "Scraped page 25\n",
            "Scraped page 26\n",
            "Scraped page 27\n",
            "Scraped page 28\n",
            "Scraped page 29\n",
            "Scraped page 30\n",
            "Scraped page 31\n",
            "Scraped page 32\n",
            "Scraped page 33\n",
            "Scraped page 34\n",
            "Scraped page 35\n",
            "Scraped page 36\n",
            "Scraped page 37\n",
            "Scraped page 38\n",
            "Scraped page 39\n",
            "Scraped page 40\n",
            "Scraped page 41\n",
            "Scraped page 42\n",
            "Scraped page 43\n",
            "Scraped page 44\n",
            "Scraped page 45\n",
            "Scraped page 46\n",
            "Scraped page 47\n",
            "Scraped page 48\n",
            "Scraped page 49\n",
            "Scraped page 50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f621fe50-b352-48a4-9dec-9d9cd7973903\", \"publications.xlsx\", 56535)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MmqbFz00NOpl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}